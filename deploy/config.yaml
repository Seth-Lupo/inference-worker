# =============================================================================
# Centralized Build Configuration
# =============================================================================
# All build scripts read from this file. Edit here instead of in scripts.
# Environment variables override these values (e.g., HF_TOKEN, QUANTIZATION)
# =============================================================================

# -----------------------------------------------------------------------------
# Docker Images - MUST USE MATCHING VERSIONS
# -----------------------------------------------------------------------------
# TensorRT version compatibility is critical:
# - Build and runtime containers must have the same TensorRT version
# - Using NGC 24.12 series for all containers
images:
  # TensorRT container for ONNX->TRT builds (trtexec)
  tensorrt: "nvcr.io/nvidia/tensorrt:24.12-py3"

  # Triton container for serving (must match TRT version)
  triton: "nvcr.io/nvidia/tritonserver:24.12-py3"

  # vLLM container for LLM inference
  vllm: "vllm/vllm-openai:latest"

# -----------------------------------------------------------------------------
# TensorRT Build Settings
# -----------------------------------------------------------------------------
tensorrt:
  # Default workspace memory in MiB
  workspace_mib: 4096

  # Large model workspace (flow decoder, etc.)
  large_workspace_mib: 8192

  # Default precision for TRT engine builds
  default_precision: "fp16"

# -----------------------------------------------------------------------------
# Triton Paths (as seen inside container)
# -----------------------------------------------------------------------------
triton_paths:
  # Base model directory mount point
  base: "/models"

  # Model subdirectories
  tts: "/models/tts"

# -----------------------------------------------------------------------------
# vLLM: Qwen3-4B with INT4 AWQ Quantization
# -----------------------------------------------------------------------------
vllm:
  # Model to serve (AWQ pre-quantized from HuggingFace)
  model: "cpatonn/Qwen3-4B-Instruct-2507-AWQ-4bit"

  # Quantization method (awq, gptq, or none)
  quantization: "awq"

  # Engine settings (native context: 262K, limited for memory efficiency)
  max_model_len: 8192
  gpu_memory_utilization: 0.85

  # Optimization
  enable_prefix_caching: true
  tensor_parallel_size: 1

  # API settings
  port: 8000
  api_key: ""  # Optional, leave empty for no auth

# -----------------------------------------------------------------------------
# TTS: Chatterbox Turbo (350M, TensorRT + ONNX Runtime)
# -----------------------------------------------------------------------------
# Architecture:
# - embed_tokens: TensorRT (simple embedding layer)
# - language_model: ONNX Runtime (complex KV cache operations)
# - speech_encoder: ONNX Runtime (STFT custom op - not TRT compatible)
# - conditional_decoder: ONNX Runtime (MultiHeadAttention custom op)
# -----------------------------------------------------------------------------
chatterbox:
  # Model source (ONNX exports from HuggingFace)
  hf_repo: "ResembleAI/chatterbox-turbo-ONNX"

  # Precision for models: fp16 or fp32
  precision: "fp16"

  # Native sample rate
  sample_rate: 24000

  # Triton settings
  max_batch_size: 8
  max_concurrent: 4     # KV cache slots for concurrent generation
  decoupled_mode: true  # Enable streaming

  # Generation parameters
  temperature: 0.8
  top_p: 0.95
  top_k: 50
  repetition_penalty: 1.2
  token_chunk_size: 25  # Tokens per audio chunk (~1 second)

  # Triton paths (inside container)
  assets_path: "/models/tts/chatterbox_assets"

  # Model-specific backends
  # TensorRT: embed_tokens (simple ops)
  # ONNX Runtime: everything else (custom ops not supported by TRT)
  backends:
    embed_tokens: "tensorrt"      # Simple embedding - TRT works
    language_model: "onnxruntime" # Complex KV cache - keep as ONNX
    speech_encoder: "onnxruntime" # STFT op not supported by TRT
    conditional_decoder: "onnxruntime" # MultiHeadAttention not supported

  # Dynamic shapes for TensorRT engine builds
  shapes:
    embed_tokens:
      min: "input_ids:1x1"
      opt: "input_ids:4x256"
      max: "input_ids:8x1024"
