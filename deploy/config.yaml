# =============================================================================
# Centralized Build Configuration
# =============================================================================
# All build scripts read from this file. Edit here instead of in scripts.
# Environment variables override these values (e.g., HF_TOKEN, QUANTIZATION)
# =============================================================================

# -----------------------------------------------------------------------------
# Docker Images - MUST USE MATCHING VERSIONS
# -----------------------------------------------------------------------------
# TensorRT version compatibility is critical:
# - Build and runtime containers must have the same TensorRT version
# - Using NGC 24.12 series for all containers
images:
  # TensorRT container for ONNX->TRT builds (trtexec)
  tensorrt: "nvcr.io/nvidia/tensorrt:24.08-py3"

  # Triton container for serving (must match TRT version)
  triton: "nvcr.io/nvidia/tritonserver:25.08-py3"



# -----------------------------------------------------------------------------
# TensorRT Build Settings
# -----------------------------------------------------------------------------
tensorrt:
  # Default workspace memory in MiB
  workspace_mib: 4096

  # Large model workspace (flow decoder, etc.)
  large_workspace_mib: 8192

  # Default precision for TRT engine builds
  default_precision: "fp16"

# -----------------------------------------------------------------------------
# Triton Paths (as seen inside container)
# -----------------------------------------------------------------------------
triton_paths:
  # Base model directory mount point
  base: "/models"

  # Model subdirectories
  tts: "/models/tts"

# -----------------------------------------------------------------------------
# vLLM: Qwen3-4B with INT4 AWQ Quantization
# -----------------------------------------------------------------------------
vllm:
  # Model to serve (AWQ pre-quantized from HuggingFace)
  model: "cpatonn/Qwen3-4B-Instruct-2507-AWQ-4bit"

  # Quantization method (awq, gptq, or none)
  quantization: "awq"

  # Engine settings (native context: 262K, limited for memory efficiency)
  max_model_len: 8192
  gpu_memory_utilization: 0.85

  # Optimization
  enable_prefix_caching: true
  tensor_parallel_size: 1

  # API settings
  port: 8000
  api_key: ""  # Optional, leave empty for no auth

# -----------------------------------------------------------------------------
# TTS: Chatterbox with T3 vLLM + S3Gen PyTorch
# -----------------------------------------------------------------------------
# Architecture:
#   - T3: vLLM backend (text + conditioning → speech tokens)
#   - S3Gen: PyTorch with torch.compile() (speech tokens → audio)
#   - VoiceEncoder: PyTorch (reference audio → speaker embedding)
# -----------------------------------------------------------------------------
chatterbox:
  # HuggingFace repo (weights only, no ONNX)
  hf_repo: "ResembleAI/chatterbox"

  # Precision
  precision: "fp16"

  # Audio settings
  sample_rate: 24000

  # Use torch.compile() for optimization
  use_torch_compile: true

  # Triton paths (inside container)
  assets_path: "/models/tts/chatterbox_assets"
  t3_weights_path: "/models/t3_weights"

# -----------------------------------------------------------------------------
# T3: Speech Token Generator (vLLM Backend)
# -----------------------------------------------------------------------------
t3:
  # vLLM settings
  gpu_memory_utilization: 0.3
  max_model_len: 2048
  dtype: "float16"
  trust_remote_code: true
  enforce_eager: true  # Disable CUDA graphs for custom model

# -----------------------------------------------------------------------------
# S3Gen: Flow Matching Decoder + HiFT Vocoder (Pure PyTorch)
# -----------------------------------------------------------------------------
s3gen:
  # All components run in PyTorch with torch.compile()
  # - Flow decoder: speech tokens → mel spectrogram
  # - HiFT vocoder: mel → audio waveform

  # Progressive streaming schedule
  # Early chunks use fewer diffusion steps for lower latency
  # Later chunks use more steps for better quality
  progressive_chunks: [4, 8, 16, 32, 32, 32, 32, 32]  # Tokens per chunk
  progressive_steps: [2, 3, 5, 7, 7, 7, 7, 7]         # Diffusion steps per chunk

  # Steady state (after progressive schedule exhausted)
  steady_chunk_size: 32
  steady_diffusion_steps: 7
