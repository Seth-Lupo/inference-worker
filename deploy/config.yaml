# =============================================================================
# Centralized Build Configuration
# =============================================================================
# All build scripts read from this file. Edit here instead of in scripts.
# Environment variables override these values (e.g., HF_TOKEN, QUANTIZATION)
# =============================================================================

# -----------------------------------------------------------------------------
# Docker Images
# -----------------------------------------------------------------------------
images:
  # Triton container with vLLM + Python backends
  triton: "nvcr.io/nvidia/tritonserver:25.05-vllm-python-py3"

# -----------------------------------------------------------------------------
# Triton Paths (as seen inside container)
# -----------------------------------------------------------------------------
triton_paths:
  # Base model directory mount point
  base: "/models"

  # Model subdirectories
  tts: "/models/tts"

# -----------------------------------------------------------------------------
# vLLM: Qwen3-4B with INT4 GPTQ Quantization
# -----------------------------------------------------------------------------
vllm:
  # Model to serve (GPTQ pre-quantized from HuggingFace)
  model: "JunHowie/Qwen3-4B-GPTQ-Int4"

  # Quantization method (awq, gptq, or none)
  quantization: "gptq"

  # Engine settings (native context: 262K, limited for memory efficiency)
  max_model_len: 8192
  gpu_memory_utilization: 0.85

  # Optimization
  enable_prefix_caching: true
  tensor_parallel_size: 1

  # API settings
  port: 8000
  api_key: ""  # Optional, leave empty for no auth

# -----------------------------------------------------------------------------
# TTS: Chatterbox with T3 vLLM + S3Gen PyTorch
# -----------------------------------------------------------------------------
# Architecture:
#   - T3: vLLM backend (text + conditioning → speech tokens)
#   - S3Gen: PyTorch with torch.compile() (speech tokens → audio)
#   - VoiceEncoder: PyTorch (reference audio → speaker embedding)
# -----------------------------------------------------------------------------
chatterbox:
  # HuggingFace repo (weights only, no ONNX)
  hf_repo: "ResembleAI/chatterbox"

  # Precision
  precision: "fp16"

  # Audio settings
  sample_rate: 24000

  # Use torch.compile() for optimization
  use_torch_compile: true

  # Triton paths (inside container)
  assets_path: "/models/tts/chatterbox_assets"
  t3_weights_path: "/models/t3_weights"

# -----------------------------------------------------------------------------
# T3: Speech Token Generator (vLLM Backend)
# -----------------------------------------------------------------------------
t3:
  # vLLM settings
  gpu_memory_utilization: 0.3
  max_model_len: 2048
  dtype: "float16"
  trust_remote_code: true
  enforce_eager: true  # Disable CUDA graphs for custom model

# -----------------------------------------------------------------------------
# S3Gen: Flow Matching Decoder + HiFT Vocoder (Pure PyTorch)
# -----------------------------------------------------------------------------
s3gen:
  # All components run in PyTorch with torch.compile()
  # - Flow decoder: speech tokens → mel spectrogram
  # - HiFT vocoder: mel → audio waveform

  # Progressive streaming schedule
  # Early chunks use fewer diffusion steps for lower latency
  # Later chunks use more steps for better quality
  progressive_chunks: [4, 8, 16, 32, 32, 32, 32, 32]  # Tokens per chunk
  progressive_steps: [2, 3, 5, 7, 7, 7, 7, 7]         # Diffusion steps per chunk

  # Steady state (after progressive schedule exhausted)
  steady_chunk_size: 32
  steady_diffusion_steps: 7
