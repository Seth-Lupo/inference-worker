# =============================================================================
# Centralized Build Configuration
# =============================================================================
# All build scripts read from this file. Edit here instead of in scripts.
# Environment variables override these values (e.g., HF_TOKEN, QUANTIZATION)
# =============================================================================

# -----------------------------------------------------------------------------
# Docker Images - MUST USE MATCHING VERSIONS
# -----------------------------------------------------------------------------
# TensorRT version compatibility is critical:
# - Build and runtime containers must have the same TensorRT version
# - Using NGC 24.12 series for all containers
images:
  # TensorRT container for ONNX->TRT builds (trtexec)
  tensorrt: "nvcr.io/nvidia/tensorrt:24.12-py3"

  # Triton container for serving (must match TRT version)
  triton: "nvcr.io/nvidia/tritonserver:24.12-py3"

  # vLLM container for LLM inference
  vllm: "vllm/vllm-openai:latest"

# -----------------------------------------------------------------------------
# TensorRT Build Settings
# -----------------------------------------------------------------------------
tensorrt:
  # Default workspace memory in MiB
  workspace_mib: 4096

  # Large model workspace (flow decoder, etc.)
  large_workspace_mib: 8192

  # Default precision for TRT engine builds
  default_precision: "fp16"

# -----------------------------------------------------------------------------
# Triton Paths (as seen inside container)
# -----------------------------------------------------------------------------
triton_paths:
  # Base model directory mount point
  base: "/models"

  # Model subdirectories
  tts: "/models/tts"

# -----------------------------------------------------------------------------
# vLLM: Qwen3-4B with INT4 AWQ Quantization
# -----------------------------------------------------------------------------
vllm:
  # Model to serve (AWQ pre-quantized from HuggingFace)
  model: "cpatonn/Qwen3-4B-Instruct-2507-AWQ-4bit"

  # Quantization method (awq, gptq, or none)
  quantization: "awq"

  # Engine settings (native context: 262K, limited for memory efficiency)
  max_model_len: 8192
  gpu_memory_utilization: 0.85

  # Optimization
  enable_prefix_caching: true
  tensor_parallel_size: 1

  # API settings
  port: 8000
  api_key: ""  # Optional, leave empty for no auth

# -----------------------------------------------------------------------------
# TTS: Chatterbox with T3 vLLM + S3Gen PyTorch
# -----------------------------------------------------------------------------
# Architecture:
#   - T3: vLLM backend (text + conditioning → speech tokens)
#   - S3Gen: PyTorch (speech tokens → audio via flow matching + HiFiGAN)
#   - VoiceEncoder: PyTorch (reference audio → speaker embedding)
#   - Vocoder: Optional TensorRT for HiFiGAN acceleration
# -----------------------------------------------------------------------------
chatterbox:
  # HuggingFace repos
  hf_repo: "ResembleAI/chatterbox"           # Main repo (weights)
  hf_repo_onnx: "ResembleAI/chatterbox-turbo-ONNX"  # ONNX for TRT builds

  # Precision
  precision: "fp16"

  # Audio settings
  sample_rate: 24000

  # Triton paths (inside container)
  assets_path: "/models/tts/chatterbox_assets"
  t3_weights_path: "/models/t3_weights"

# -----------------------------------------------------------------------------
# T3: Speech Token Generator (vLLM Backend)
# -----------------------------------------------------------------------------
t3:
  # vLLM settings
  gpu_memory_utilization: 0.3
  max_model_len: 2048
  dtype: "float16"
  trust_remote_code: true
  enforce_eager: true  # Disable CUDA graphs for custom model

# -----------------------------------------------------------------------------
# S3Gen: Flow Matching Decoder + Vocoder
# -----------------------------------------------------------------------------
s3gen:
  # Flow matching (diffusion) settings
  # Fixed steps for TensorRT, or variable for PyTorch
  diffusion_steps: 4          # Fixed steps (for TRT unrolled engine)
  diffusion_steps_trt: 4      # Steps baked into TRT engine

  # Progressive streaming (PyTorch only - variable steps per chunk)
  # First chunks use fewer steps for lower latency
  progressive_chunks: [4, 8, 16, 32, 32]    # Tokens per chunk
  progressive_steps: [1, 2, 5, 7, 10]       # Diffusion steps per chunk

  # Vocoder backend: "trt" or "pytorch"
  # TRT is faster but requires building the engine
  vocoder_backend: "trt"

  # TensorRT vocoder shapes (for conditional_decoder)
  vocoder_shapes:
    min: "speech_tokens:1x4"
    opt: "speech_tokens:1x32"
    max: "speech_tokens:1x64"

  # Flow decoder TRT (optional - unrolled N steps)
  # Set to true to build TRT engine with fixed diffusion_steps_trt
  flow_decoder_trt: false     # Not yet implemented
  flow_decoder_shapes:
    min: "latents:1x256x16"
    opt: "latents:1x256x64"
    max: "latents:1x256x128"
