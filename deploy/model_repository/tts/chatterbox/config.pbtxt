# Chatterbox Turbo TTS - Python Backend with vLLM T3
# Uses vLLM internally for speech token generation
# Streaming audio output via decoupled mode

name: "chatterbox"
backend: "python"
max_batch_size: 0

input [
  {
    name: "text"
    data_type: TYPE_STRING
    dims: [1, 1]
  },
  {
    name: "voice_id"
    data_type: TYPE_STRING
    dims: [1]
    optional: true
  },
  {
    name: "reference_audio"
    data_type: TYPE_FP32
    dims: [-1]
    optional: true
  },
  {
    name: "exaggeration"
    data_type: TYPE_FP32
    dims: [1]
    optional: true
  }
]

output [
  {
    name: "audio"
    data_type: TYPE_FP32
    dims: [-1]
  }
]

# Decoupled mode enables streaming audio responses
model_transaction_policy {
  decoupled: True
}

# Single instance - vLLM manages its own GPU resources
instance_group [
  {
    count: 1
    kind: KIND_GPU
  }
]

parameters {
  key: "model_dir"
  value: { string_value: "/models/tts/chatterbox_assets" }
}

parameters {
  key: "sample_rate"
  value: { string_value: "24000" }
}

# vLLM T3 configuration
parameters {
  key: "t3_max_model_len"
  value: { string_value: "1000" }
}

parameters {
  key: "t3_gpu_memory_utilization"
  value: { string_value: "0.3" }
}
