version: "3.8"

# =============================================================================
# Voice Agent Inference Stack
#
# Architecture:
# - Triton Inference Server with vLLM backend (all models in one server)
#   - LLM: Qwen3-8B-AWQ (vLLM backend) - chat/reasoning
#   - LLM: T3 Turbo (vLLM backend) - speech token generation
#   - TTS: Chatterbox (Python backend) - calls T3 via gRPC, runs S3Gen locally
#   - ASR: Parakeet (Python backend) - TensorRT
# - Worker (Port 80): WebSocket server
#
# T3 runs as a native vLLM backend model (not wrapped in Python).
# Chatterbox calls T3 via Triton gRPC for token generation.
# Container: tritonserver:24.12-vllm-python-py3
# =============================================================================

services:
  # =============================================================================
  # Triton Inference Server (with vLLM Backend)
  # Serves ALL models: LLMs via vLLM, TTS/ASR via Python/TensorRT
  # =============================================================================
  triton:
    image: nvcr.io/nvidia/tritonserver:24.12-vllm-python-py3
    container_name: voice-agent-triton
    restart: unless-stopped

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]

    shm_size: "16g"
    ulimits:
      memlock: -1
      stack: 67108864

    ports:
      - "8000:8000"   # HTTP (OpenAI-compatible for vLLM models)
      - "8001:8001"   # gRPC
      - "8002:8002"   # Metrics

    volumes:
      # Model repositories
      - ./model_repository/llm:/models/llm
      - ./model_repository/tts:/models/tts
      - ./model_repository/asr:/models/asr
      # T3 weights (downloaded by build script, used by vLLM backend)
      - ./models/t3_weights:/models/t3_weights:ro
      # HuggingFace cache for Qwen3 download
      - ${HF_HOME:-~/.cache/huggingface}:/root/.cache/huggingface
      # Logs
      - ./logs/triton:/var/log/triton

    environment:
      - CUDA_VISIBLE_DEVICES=0
      - HF_TOKEN=${HF_TOKEN}
      - TRT_LOGGER_LEVEL=WARNING
      # vLLM settings
      - VLLM_ATTENTION_BACKEND=FLASHINFER
      # Chatterbox settings (Python backend calls T3 via gRPC)
      - TRITON_GRPC_URL=localhost:8001
      - T3_MODEL_NAME=t3
      # Progressive streaming for low first-chunk latency
      - TTS_PROGRESSIVE_CHUNKS=4,8,16,32,32
      - TTS_PROGRESSIVE_STEPS=1,2,5,7,10
      - TTS_CHUNK_TOKENS=32
      - S3GEN_DIFFUSION_STEPS=10

    command: >
      tritonserver
      --model-repository=/models/llm
      --model-repository=/models/tts
      --model-repository=/models/asr
      --model-control-mode=explicit
      --load-model=qwen3
      --load-model=t3
      --load-model=chatterbox
      --load-model=parakeet_tdt
      --disable-auto-complete-config
      --http-port=8000
      --grpc-port=8001
      --metrics-port=8002
      --log-verbose=0

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s

  # =============================================================================
  # Voice Agent Worker
  # WebSocket server connecting to Triton for all inference
  # =============================================================================
  worker:
    build:
      context: ..
      dockerfile: deploy/Dockerfile.worker
    container_name: voice-agent-worker
    restart: unless-stopped

    ports:
      - "80:80"

    volumes:
      - ../src:/app/src
      - ./logs/worker:/app/logs
      - ../voice.wav:/app/voice.wav:ro

    environment:
      # All inference goes through Triton
      - TRITON_URL=triton:8001
      - TRITON_HTTP_URL=http://triton:8000
      # LLM model name (as registered in Triton)
      - LLM_MODEL=qwen3
      # TTS settings
      - TTS_BACKEND=chatterbox
      - TTS_MODEL=chatterbox
      # Logging
      - LOG_LEVEL=INFO
      - PYTHONUNBUFFERED=1
      # Voice cloning
      - REFERENCE_AUDIO_PATH=/app/voice.wav
      - REFERENCE_TEXT=The quick brown fox jumps over the lazy dog.

    depends_on:
      triton:
        condition: service_healthy

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  # =============================================================================
  # Model Builder (TensorRT engine compilation)
  # Usage: docker compose --profile build run model-builder
  # =============================================================================
  model-builder:
    image: nvcr.io/nvidia/tensorrt:24.12-py3
    container_name: voice-agent-model-builder
    profiles: ["build"]

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    shm_size: "8g"
    ulimits:
      memlock: -1
      stack: 67108864

    volumes:
      - ./model_repository:/models
      - ./models:/models_src
      - ./scripts:/scripts:ro
      - ./chatterbox_build:/chatterbox_build
      - ./vllm_models:/vllm_models
      - ${HF_HOME:-~/.cache/huggingface}:/root/.cache/huggingface

    working_dir: /scripts

    environment:
      - HF_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0

    command: ["bash"]
    stdin_open: true
    tty: true

networks:
  default:
    name: voice-agent-network
