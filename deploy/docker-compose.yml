version: "3.8"

services:
  # =============================================================================
  # Triton Inference Server with TensorRT-LLM Backend
  # Models: qwen3_8b, parakeet_tdt, cosyvoice2 (7 submodels)
  # =============================================================================
  triton:
    # Triton with TensorRT-LLM backend
    build:
      context: ..
      dockerfile: deploy/Dockerfile.triton
    container_name: voice-agent-triton
    restart: unless-stopped

    # GPU access
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    # Shared memory for TensorRT-LLM
    shm_size: "8g"
    ulimits:
      memlock: -1
      stack: 67108864

    ports:
      - "8000:8000"   # HTTP
      - "8001:8001"   # gRPC
      - "8002:8002"   # Metrics

    volumes:
      - ./model_repository:/models
      - ../src/cosyvoice:/opt/cosyvoice:ro
      - ./engine_cache:/engine_cache
      - ./logs/triton:/var/log/triton
      - ./qwen3_build:/qwen3_build
      - ./cosyvoice_build:/cosyvoice_build

    environment:
      - CUDA_VISIBLE_DEVICES=0
      - TRT_LOGGER_LEVEL=WARNING
      - PYTHONPATH=/opt

    command: >
      tritonserver
      --model-repository=/models/tts
      --model-repository=/models/llm
      --model-repository=/models/asr
      --model-control-mode=explicit
      --load-model=qwen3_8b
      --load-model=parakeet_tdt
    
      --strict-model-config=false
      --http-port=8000
      --grpc-port=8001
      --metrics-port=8002

    healthcheck:
        test: ["CMD", "true"]
        interval: 30s
        timeout: 10s
        retries: 5
        start_period: 1000s

  # =============================================================================
  # Voice Agent Inference Worker (Python WebSocket Server)
  # =============================================================================
  worker:
    build:
      context: ..
      dockerfile: deploy/Dockerfile.worker
    container_name: voice-agent-worker
    restart: unless-stopped

    ports:
      - "80:80"

    volumes:
      - ../src:/app/src
      - ./logs/worker:/app/logs

    environment:
      - TRITON_URL=triton:8001
      - TRITON_HTTP_URL=http://triton:8000
      - TTS_BACKEND=cosyvoice
      - TTS_MODEL=cosyvoice2
      - LOG_LEVEL=INFO
      - PYTHONUNBUFFERED=1

    depends_on:
      triton:
        condition: service_healthy

    healthcheck:
      test: ["CMD", "true"]
      interval: 10s
      timeout: 5s
      retries: 3

  # =============================================================================
  # Model Builder (One-off container for building TensorRT engines)
  # =============================================================================
  model-builder:
    image: docker
    container_name: voice-agent-model-builder
    profiles: ["build"]  # Only runs with: docker compose --profile build up model-builder

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    shm_size: "8g"
    ulimits:
      memlock: -1
      stack: 67108864

    volumes:
      - ./model_repository:/models
      - ./engine_cache:/engine_cache
      - ./scripts:/scripts:ro
      - ./model_sources:/model_sources

    working_dir: /scripts

    environment:
      - HF_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0

    command: ["bash", "-c", "echo 'Model builder ready. Run scripts manually or use: docker exec -it voice-agent-model-builder bash'"]

    stdin_open: true
    tty: true

networks:
  default:
    name: voice-agent-network
