version: "3.8"

# =============================================================================
# Voice Agent Inference Stack
#
# Architecture:
# - Triton Inference Server with vLLM backend (all models in one server)
#   - LLM: Qwen3-4B-AWQ (vLLM backend) - chat/reasoning
#   - LLM: T3 Turbo (vLLM backend) - speech token generation
#   - TTS: Chatterbox (Python backend) - calls T3 via gRPC, runs S3Gen locally
#   - ASR: Parakeet (BLS + ONNX Runtime GPU backends)
# - Worker (Port 80): WebSocket server
#
# T3 runs as a native vLLM backend model (not wrapped in Python).
# Chatterbox calls T3 via Triton gRPC for token generation.
# Parakeet uses ONNX Runtime GPU for encoder/decoder inference.
# Container: tritonserver:25.05-vllm-python-py3 (requires Driver 575+)
# =============================================================================

services:
  # =============================================================================
  # Triton Inference Server (with vLLM Backend)
  # Serves ALL models: LLMs via vLLM, TTS/ASR via Python backends
  # =============================================================================
  triton:
    image: nvcr.io/nvidia/tritonserver:25.04-py3
    container_name: voice-agent-triton
    restart: unless-stopped
    build:
      context: ..
      dockerfile: deploy/Dockerfile.triton

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]

    shm_size: "16g"
    ulimits:
      memlock: -1
      stack: 67108864

    ports:
      - "8000:8000"   # HTTP (OpenAI-compatible for vLLM models)
      - "8001:8001"   # gRPC
      - "8002:8002"   # Metrics

    volumes:
      # Model repositories
      - ./model_repository/llm:/models/llm
      - ./model_repository/tts:/models/tts
      - ./model_repository/asr:/models/asr
      # T3 weights (downloaded by build script, used by vLLM backend)
      - ./models/t3_weights:/models/t3_weights:ro
      # Qwen3 weights (downloaded by build script, used by vLLM backend)
      - ./models/qwen3_weights:/models/qwen3_weights:ro
      # HuggingFace cache (fallback)
      - ${HF_HOME:-~/.cache/huggingface}:/root/.cache/huggingface
      # Logs
      - ./logs/triton:/var/log/triton

    environment:
      - CUDA_VISIBLE_DEVICES=0
      - HF_TOKEN=${HF_TOKEN}
      # vLLM settings - V1 engine cannot run in Triton's background threads
      - VLLM_USE_V1=0
      - VLLM_ATTENTION_BACKEND=FLASHINFER
      # Chatterbox settings (Python backend calls T3 via gRPC)
      - TRITON_GRPC_URL=localhost:8001
      - T3_MODEL_NAME=t3
      # Progressive streaming for low first-chunk latency
      - TTS_PROGRESSIVE_CHUNKS=4,8,16,32,32,32,32,32
      - TTS_PROGRESSIVE_STEPS=2,3,5,7,7,7,7,7
      - TTS_CHUNK_TOKENS=32
      - S3GEN_DIFFUSION_STEPS=7
      # torch.compile for S3Gen optimization
      - USE_TORCH_COMPILE=true

    command: >
      tritonserver
      --model-repository=/models/llm
      --model-repository=/models/tts
      --model-repository=/models/asr
      --model-control-mode=explicit
      --model-load-thread-count=1
      --load-model=parakeet_decoder
      --load-model=parakeet_encoder
      --load-model=parakeet_tdt
      --load-model=chatterbox_s3gen
      --load-model=chatterbox_voice_encoder
      --load-model=chatterbox
      --load-model=qwen3
      --load-model=t3
      --disable-auto-complete-config
      --http-port=8000
      --grpc-port=8001
      --metrics-port=8002
      --log-verbose=0

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s

  # =============================================================================
  # Voice Agent Worker
  # WebSocket server connecting to Triton for all inference
  # =============================================================================
  worker:
    build:
      context: ..
      dockerfile: deploy/Dockerfile.worker
    container_name: voice-agent-worker
    restart: unless-stopped

    ports:
      - "80:80"

    volumes:
      - ../src:/app/src
      - ./logs/worker:/app/logs
      - ../voice.wav:/app/voice.wav:ro

    environment:
      # All inference goes through Triton
      - TRITON_URL=triton:8001
      - TRITON_HTTP_URL=http://triton:8000
      # LLM model name (as registered in Triton)
      - LLM_MODEL=qwen3
      # TTS settings
      - TTS_BACKEND=chatterbox
      - TTS_MODEL=chatterbox
      # Logging
      - LOG_LEVEL=INFO
      - PYTHONUNBUFFERED=1
      # Voice cloning
      - REFERENCE_AUDIO_PATH=/app/voice.wav
      - REFERENCE_TEXT=The quick brown fox jumps over the lazy dog.

    depends_on:
      triton:
        condition: service_healthy

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 10s
      timeout: 5s
      retries: 3

networks:
  default:
    name: voice-agent-network
