version: "3.8"

# =============================================================================
# Voice Agent Inference Stack - Pure Native Backends
#
# Architecture:
# - Triton: Pure inference only (native backends, no Python)
#   - LLM: Qwen3-4B-AWQ (vLLM backend)
#   - TTS: T3 Turbo (vLLM backend) - speech token generation
#   - TTS: speech_encoder (ONNX) - reference audio conditioning
#   - TTS: conditional_decoder (ONNX) - speech tokens to audio
#   - ASR: Parakeet encoder/decoder (native ONNX backend)
#
# - Worker: Orchestration layer
#   - Audio preprocessing (VAD, resampling)
#   - ASR decoding loop (greedy TDT decoding)
#   - TTS pipeline orchestration (calls Triton models)
#
# Worker calls Triton via gRPC for pure tensor inference.
# =============================================================================

services:
  # =============================================================================
  # Triton Inference Server - Pure Native Backends
  # vLLM for T3/Qwen3, ONNX Runtime for ASR and TTS encoder/decoder
  # =============================================================================
  triton:
    build:
      context: ..
      dockerfile: deploy/Dockerfile.triton
    image: voice-agent-triton:latest
    container_name: voice-agent-triton
    restart: unless-stopped

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]

    shm_size: "16g"
    ulimits:
      memlock: -1
      stack: 67108864

    ports:
      - "8000:8000"   # HTTP (OpenAI-compatible for vLLM models)
      - "8001:8001"   # gRPC
      - "8002:8002"   # Metrics

    volumes:
      # Model repositories (configs for native backends)
      - ./model_repository/llm:/models/llm
      - ./model_repository/tts:/models/tts
      - ./model_repository/asr:/models/asr
      # Chatterbox TTS ONNX models (keep original filenames for external data references)
      - ./models/chatterbox_onnx/speech_encoder_fp16.onnx:/models/tts/speech_encoder/1/speech_encoder_fp16.onnx
      - ./models/chatterbox_onnx/speech_encoder_fp16.onnx_data:/models/tts/speech_encoder/1/speech_encoder_fp16.onnx_data
      - ./models/chatterbox_onnx/conditional_decoder_fp16.onnx:/models/tts/conditional_decoder/1/conditional_decoder_fp16.onnx
      - ./models/chatterbox_onnx/conditional_decoder_fp16.onnx_data:/models/tts/conditional_decoder/1/conditional_decoder_fp16.onnx_data
      # Parakeet ASR ONNX models (keep original names for external data references)
      - ./models/parakeet_onnx/encoder-model.onnx:/models/asr/parakeet_encoder/1/encoder-model.onnx
      - ./models/parakeet_onnx/encoder-model.onnx.data:/models/asr/parakeet_encoder/1/encoder-model.onnx.data
      - ./models/parakeet_onnx/decoder_joint-model.onnx:/models/asr/parakeet_decoder/1/model.onnx
      # vLLM model weights
      - ./models/t3_weights:/models/t3_weights
      - ./models/qwen3_weights:/models/qwen3_weights
      # HuggingFace cache (fallback)
      - ${HF_HOME:-~/.cache/huggingface}:/root/.cache/huggingface
      # Logs
      - ./logs/triton:/var/log/triton

    environment:
      - CUDA_VISIBLE_DEVICES=0
      - HF_TOKEN=${HF_TOKEN}
      # vLLM settings - V1 engine cannot run in Triton's background threads
      - VLLM_USE_V1=0
      - VLLM_ATTENTION_BACKEND=FLASHINFER

    command: >
      tritonserver
      --model-repository=/models/llm
      --model-repository=/models/tts
      --model-repository=/models/asr
      --model-control-mode=explicit
      --model-load-thread-count=1
      --load-model=t3
      --disable-auto-complete-config
      --http-port=8000
      --grpc-port=8001
      --metrics-port=8002
      --log-verbose=0

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s

  # =============================================================================
  # Voice Agent Worker
  # WebSocket server with orchestration logic
  # - ASR: Mel extraction + greedy TDT decoding (calls Triton encoder/decoder)
  # - TTS: Calls Triton for all inference (speech_encoder, T3, conditional_decoder)
  # =============================================================================
  worker:
    build:
      context: ..
      dockerfile: deploy/Dockerfile.worker
    container_name: voice-agent-worker
    restart: unless-stopped

    ports:
      - "80:80"

    volumes:
      - ../src:/app/src
      - ./logs/worker:/app/logs
      - ../voice.wav:/app/voice.wav:ro

    environment:
      # Triton connection (all inference via gRPC)
      - TRITON_URL=triton:8001
      - TRITON_HTTP_URL=http://triton:8000
      # LLM model name
      - LLM_MODEL=qwen3
      # TTS settings (all models via Triton ONNX/vLLM)
      - TTS_SPEECH_ENCODER_MODEL=speech_encoder
      - TTS_T3_MODEL=t3
      - TTS_DECODER_MODEL=conditional_decoder
      - TTS_SAMPLE_RATE=24000
      # ASR settings (encoder/decoder via Triton ONNX)
      - ASR_ENCODER_MODEL=parakeet_encoder
      - ASR_DECODER_MODEL=parakeet_decoder
      # Logging
      - LOG_LEVEL=INFO
      - PYTHONUNBUFFERED=1
      # Voice cloning
      - REFERENCE_AUDIO_PATH=/app/voice.wav
      - REFERENCE_TEXT=The quick brown fox jumps over the lazy dog.

    depends_on:
      triton:
        condition: service_healthy

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 10s
      timeout: 5s
      retries: 3

networks:
  default:
    name: voice-agent-network
