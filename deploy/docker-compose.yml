version: "3.8"

services:
  # =============================================================================
  # Triton Inference Server with TensorRT-LLM Backend
  # =============================================================================
  triton:
    # Custom Triton image with TensorRT-LLM + Python backends + sherpa-onnx
    build:
      context: .
      dockerfile: Dockerfile.triton
    image: voice-agent-triton:latest
    container_name: voice-agent-triton
    restart: unless-stopped

    # GPU access
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    # Shared memory for TensorRT-LLM
    shm_size: "8g"
    ulimits:
      memlock: -1
      stack: 67108864

    ports:
      - "8000:8000"   # HTTP
      - "8001:8001"   # gRPC
      - "8002:8002"   # Metrics

    volumes:
      # Model repository (parakeet_tdt, qwen3_8b)
      # Note: silero_vad runs locally in worker, not in Triton
      - ./model_repository:/models:rw
      # Mount built Qwen3 TensorRT-LLM engine
      - ./qwen3_build/engine_int4:/models/qwen3_8b/1/engine:ro
      # Mount CosyVoice2 full model repo (5 models with Python backends)
      # Built by: ./scripts/build_cosyvoice.sh -1 2
      # Note: rw needed for TensorRT engine caching
      - ./model_repository/cosyvoice2_full:/models/cosyvoice2_full:rw
      # Engine cache
      - ./engine_cache:/engine_cache
      # Logs
      - ./logs/triton:/var/log/triton

    environment:
      - CUDA_VISIBLE_DEVICES=0
      - TRT_LOGGER_LEVEL=WARNING

    command: >
      tritonserver
      --model-repository=/models
      --model-repository=/models/cosyvoice2_full
      --model-control-mode=explicit
      --load-model=qwen3_8b
      --load-model=cosyvoice2
      --load-model=audio_tokenizer
      --load-model=speaker_embedding
      --load-model=tensorrt_llm
      --load-model=token2wav
      --load-model=parakeet_tdt
      --strict-model-config=false
      --http-port=8000
      --grpc-port=8001
      --metrics-port=8002

    healthcheck:
        test: ["CMD", "true"]
        interval: 30s
        timeout: 10s
        retries: 5
        start_period: 1000s

  # =============================================================================
  # Voice Agent Inference Worker (Python WebSocket Server)
  # =============================================================================
  worker:
    build:
      context: ..
      dockerfile: deploy/Dockerfile.worker
    container_name: voice-agent-worker
    restart: unless-stopped

    ports:
      - "80:80"

    volumes:
      # Sync source code for development
      - ../src:/app/src:ro
      # Logs
      - ./logs/worker:/app/logs

    environment:
      - TRITON_URL=triton:8001
      - TRITON_HTTP_URL=http://triton:8000
      - TTS_BACKEND=cosyvoice
      - TTS_MODEL=cosyvoice2
      - LOG_LEVEL=INFO
      - PYTHONUNBUFFERED=1

    depends_on:
      triton:
        condition: service_healthy

    healthcheck:
      test: ["CMD", "true"]
      interval: 10s
      timeout: 5s
      retries: 3

  # =============================================================================
  # Model Builder (One-off container for building TensorRT engines)
  # =============================================================================
  model-builder:
    image: docker
    container_name: voice-agent-model-builder
    profiles: ["build"]  # Only runs with: docker compose --profile build up model-builder

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    shm_size: "8g"
    ulimits:
      memlock: -1
      stack: 67108864

    volumes:
      - ./model_repository:/models
      - ./engine_cache:/engine_cache
      - ./scripts:/scripts:ro
      - ./model_sources:/model_sources

    working_dir: /scripts

    environment:
      - HF_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0

    command: ["bash", "-c", "echo 'Model builder ready. Run scripts manually or use: docker exec -it voice-agent-model-builder bash'"]

    stdin_open: true
    tty: true

networks:
  default:
    name: voice-agent-network
