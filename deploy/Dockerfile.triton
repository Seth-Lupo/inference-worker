# Triton Inference Server with TensorRT + ONNX Runtime + Python backends
# For serving Chatterbox TTS models
FROM nvcr.io/nvidia/tritonserver:24.12-py3

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
        espeak-ng \
        libsndfile1 \
    && rm -rf /var/lib/apt/lists/*

# Get torch version from base image, install matching torchaudio/torchvision
RUN TORCH_VERSION=$(pip show torch | grep Version | cut -d' ' -f2) && \
    CUDA_VERSION=$(python3 -c "import torch; print(torch.version.cuda.replace('.', '')[:3])") && \
    echo "Base image: torch==${TORCH_VERSION} CUDA=${CUDA_VERSION}" && \
    pip install --no-cache-dir --no-deps \
        torchaudio torchvision \
        --index-url https://download.pytorch.org/whl/cu${CUDA_VERSION} && \
    echo "torch==${TORCH_VERSION}" > /tmp/constraints.txt && \
    echo "torchaudio==$(pip show torchaudio | grep Version | cut -d' ' -f2)" >> /tmp/constraints.txt && \
    echo "torchvision==$(pip show torchvision | grep Version | cut -d' ' -f2)" >> /tmp/constraints.txt && \
    cat /tmp/constraints.txt

# Install grpcio binary
RUN pip install --no-cache-dir --only-binary :all: grpcio grpcio-tools

# Install dependencies for Chatterbox TTS
RUN pip install --no-cache-dir -c /tmp/constraints.txt \
        "transformers>=4.40.0" \
        tokenizers \
        librosa soundfile \
        onnxruntime-gpu \
        onnx

# Install Python backend dependencies
RUN pip install --no-cache-dir -c /tmp/constraints.txt \
        numpy scipy
