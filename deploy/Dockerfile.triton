# =============================================================================
# Triton Inference Server - Pure Native Backends
# =============================================================================
#
# Models:
#   - qwen3: vLLM backend (LLM for chat/reasoning)
#   - t3: vLLM backend (TTS speech token generation)
#   - speech_encoder: ONNX backend (TTS reference audio → conditioning)
#   - conditional_decoder: ONNX backend (TTS speech tokens → audio)
#   - parakeet_encoder: ONNX backend (ASR encoder)
#   - parakeet_decoder: ONNX backend (ASR decoder)
#
# ONNX models from: ResembleAI/chatterbox-turbo-ONNX
# All orchestration done in Worker via gRPC.
#
# =============================================================================

# Start with vLLM image (has vLLM backend + compatible Python packages)
FROM nvcr.io/nvidia/tritonserver:25.12-vllm-python-py3

# Copy ONNX Runtime backend from py3 image
COPY --from=nvcr.io/nvidia/tritonserver:25.12-py3 \
    /opt/tritonserver/backends/onnxruntime \
    /opt/tritonserver/backends/onnxruntime

# Environment
ENV VLLM_USE_V1=0

# Verify required backends are present
RUN echo "=== Verifying backends ===" && \
    ls -1 /opt/tritonserver/backends/ && \
    echo "" && \
    echo "Checking ONNX Runtime backend..." && \
    test -d /opt/tritonserver/backends/onnxruntime && \
    ls -la /opt/tritonserver/backends/onnxruntime/ && \
    echo "" && \
    echo "Checking vLLM backend..." && \
    test -d /opt/tritonserver/backends/vllm && \
    ls -la /opt/tritonserver/backends/vllm/ && \
    echo "" && \
    echo "=== All required backends present ==="
