# Custom Triton image with TensorRT-LLM 1.2.0rc6 + Python dependencies
# Using the TRT-LLM release container which includes Triton server
FROM nvcr.io/nvidia/tensorrt-llm/release:1.2.0rc6

# Install Python packages for Parakeet ASR and CosyVoice TTS
RUN pip install --no-cache-dir \
    sherpa-onnx \
    onnxruntime-gpu \
    numpy \
    transformers \
    sentencepiece

# The TRT-LLM release container includes:
# - TensorRT-LLM 1.2.0rc6 (matches our built engines)
# - Triton Inference Server with tensorrtllm backend
# - Python backend for custom model wrappers
