# Triton Inference Server with vLLM + Python backends
# For serving:
#   - Qwen3 LLM (vLLM backend)
#   - T3 speech token generator (vLLM backend)
#   - Chatterbox TTS (Python backend with torch.compile)
#   - Parakeet ASR (Python backend with ONNX Runtime GPU)
#
# Base image already includes: torch, transformers, huggingface_hub, tokenizers,
# safetensors, numpy (via vLLM). We only add what's missing.
#
# Note: torchaudio removed - replaced with pure torch/librosa implementations
# to avoid ABI compatibility issues with base image torch.
#
# Use vLLM container but REINSTALL vLLM to fix ABI mismatch
FROM nvcr.io/nvidia/tritonserver:25.04-vllm-python-py3

# Disable xformers to avoid triton kernel compatibility issues
ENV XFORMERS_DISABLE_MEMORY_EFFICIENT_ATTENTION=1
ENV DIFFUSERS_NO_XFORMERS=1

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
        espeak-ng \
        libsndfile1 \
        ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# Install Triton client for BLS (Python backend calling other Triton models)
RUN pip install --no-cache-dir tritonclient[grpc]

# CRITICAL FIX: Reinstall vLLM to compile against the container's PyTorch
# NVIDIA's pre-compiled vLLM has ABI mismatch with their shipped PyTorch
RUN pip uninstall -y vllm && \
    pip install --no-cache-dir vllm

# Freeze torch to prevent later packages from changing it
RUN pip freeze | grep -E "^(torch|nvidia)" > /tmp/frozen-packages.txt && \
    echo "=== Frozen packages ===" && \
    cat /tmp/frozen-packages.txt

# Install audio processing libraries
RUN pip install --no-cache-dir --constraint /tmp/frozen-packages.txt \
        librosa \
        soundfile \
        scipy

# Install S3Gen/Chatterbox dependencies
RUN pip install --no-cache-dir --constraint /tmp/frozen-packages.txt \
        omegaconf \
        einops \
        "diffusers>=0.27.0,<0.30.0" \
        conformer \
        s3tokenizer

# Remove xformers to prevent triton kernel compatibility issues
RUN pip uninstall -y xformers || true

# Install ONNX Runtime GPU for Parakeet ASR
RUN pip install --no-cache-dir --constraint /tmp/frozen-packages.txt onnxruntime-gpu

# Note: Can't verify CUDA import during build (no GPU), but vLLM reinstall should fix ABI
RUN echo "=== Installed versions ===" && \
    python3 -c "import torch; print(f'torch: {torch.__version__}')" && \
    pip show vllm | grep Version
