# Triton Inference Server with vLLM + Python backends
# For serving:
#   - Chatterbox TTS (PyTorch + torch.compile)
#   - Parakeet ASR (BLS + native TensorRT backends)
#
# Base image already includes: torch, transformers, huggingface_hub, tokenizers,
# safetensors, numpy (via vLLM). We only add what's missing.
#
# Note: torchaudio removed - replaced with pure torch/librosa implementations
# to avoid ABI compatibility issues with base image torch.
FROM nvcr.io/nvidia/tritonserver:25.12-vllm-python-py3

# Disable xformers to avoid triton kernel compatibility issues
ENV XFORMERS_DISABLE_MEMORY_EFFICIENT_ATTENTION=1
ENV DIFFUSERS_NO_XFORMERS=1

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
        espeak-ng \
        libsndfile1 \
        ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# Install Triton client for BLS (Python backend calling other Triton models)
RUN pip install --no-cache-dir tritonclient[grpc]

# Install audio processing libraries (not in base image)
# librosa replaces torchaudio for resampling
RUN pip install --no-cache-dir \
        librosa \
        soundfile \
        scipy

# Install S3Gen/Chatterbox dependencies (not in base image)
# Pin diffusers to older version to avoid xformers import issues
# Note: transformers, huggingface_hub, tokenizers, safetensors already in base via vLLM
RUN pip install --no-cache-dir \
        omegaconf \
        einops \
        "diffusers==0.25.1" \
        conformer \
        s3tokenizer
