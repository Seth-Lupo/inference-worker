# Triton Inference Server with vLLM + Python backends
#
# Models served:
#   - Qwen3 LLM (vLLM backend)
#   - T3 speech token generator (vLLM backend)
#   - Chatterbox TTS (Python backend)
#   - Parakeet ASR (Python backend + ONNX Runtime GPU)
#
# Base image includes: vLLM, torch, transformers, numpy, numba, etc.

FROM nvcr.io/nvidia/tritonserver:25.12-vllm-python-py3

# ============================================================================
# Environment Configuration
# ============================================================================

# vLLM settings
ENV VLLM_USE_V1=0

# Disable xformers (causes triton kernel issues)
ENV XFORMERS_DISABLE_MEMORY_EFFICIENT_ATTENTION=1
ENV DIFFUSERS_NO_XFORMERS=1

# ============================================================================
# System Dependencies
# ============================================================================

RUN apt-get update && apt-get install -y --no-install-recommends \
        espeak-ng \
        libsndfile1 \
        ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# ============================================================================
# Python Dependencies
# ============================================================================

# Fix numpy metadata and freeze core packages to prevent version conflicts
RUN pip install --no-cache-dir --force-reinstall numpy && \
    pip freeze | grep -E "^(torch|nvidia|numpy|numba|llvmlite)" > /tmp/frozen.txt && \
    cat /tmp/frozen.txt

# Triton client for BLS (Python backend calling other Triton models)
RUN pip install --no-cache-dir tritonclient[grpc]

# Pin transformers <4.48 to avoid aimv2 conflict with s3tokenizer
# (s3tokenizer registers model_type="aimv2", conflicts with transformers 4.48+)
RUN pip install --no-cache-dir -c /tmp/frozen.txt "transformers>=4.45.0,<4.48.0"

# Audio processing (pin numba>=0.63 to prevent downgrade)
RUN pip install --no-cache-dir -c /tmp/frozen.txt \
        "numba>=0.63.0" \
        librosa \
        soundfile \
        scipy

# Chatterbox/S3Gen dependencies
RUN pip install --no-cache-dir -c /tmp/frozen.txt \
        omegaconf \
        einops \
        conformer \
        "diffusers>=0.27.0,<0.30.0" \
        "s3tokenizer>=0.3.0"

# Parakeet ASR (ONNX Runtime GPU)
RUN pip install --no-cache-dir -c /tmp/frozen.txt onnxruntime-gpu

# Quantized model support (GPTQ, AWQ)
RUN pip install --no-cache-dir compressed-tensors

# ============================================================================
# Cleanup
# ============================================================================

RUN pip uninstall -y xformers 2>/dev/null || true && \
    rm -rf /tmp/frozen.txt /root/.cache/pip

# ============================================================================
# Verification
# ============================================================================

RUN echo "=== Package Versions ===" && \
    python3 -c "import torch; print(f'torch:        {torch.__version__}')" && \
    python3 -c "import vllm; print(f'vllm:         {vllm.__version__}')" && \
    python3 -c "import transformers; print(f'transformers: {transformers.__version__}')" && \
    python3 -c "import numpy; print(f'numpy:        {numpy.__version__}')" && \
    python3 -c "import librosa; print(f'librosa:      {librosa.__version__}')" && \
    python3 -c "import onnxruntime; print(f'onnxruntime:  {onnxruntime.__version__}')" && \
    python3 -c "import s3tokenizer; print('s3tokenizer:  OK')" && \
    echo "=== Triton Backends ===" && \
    ls -1 /opt/tritonserver/backends/
