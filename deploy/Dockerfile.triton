# Triton Inference Server with vLLM + Python backends
# For serving Chatterbox TTS with T3 vLLM backend
FROM nvcr.io/nvidia/tritonserver:24.12-vllm-python-py3

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
        espeak-ng \
        libsndfile1 \
        ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# Get torch version from base image, install matching torchaudio/torchvision
RUN TORCH_VERSION=$(pip show torch | grep Version | cut -d' ' -f2) && \
    CUDA_VERSION=$(python3 -c "import torch; print(torch.version.cuda.replace('.', '')[:3])") && \
    echo "Base image: torch==${TORCH_VERSION} CUDA=${CUDA_VERSION}" && \
    pip install --no-cache-dir --no-deps \
        torchaudio torchvision \
        --index-url https://download.pytorch.org/whl/cu${CUDA_VERSION} && \
    echo "torch==${TORCH_VERSION}" > /tmp/constraints.txt && \
    echo "torchaudio==$(pip show torchaudio | grep Version | cut -d' ' -f2)" >> /tmp/constraints.txt && \
    echo "torchvision==$(pip show torchvision | grep Version | cut -d' ' -f2)" >> /tmp/constraints.txt && \
    cat /tmp/constraints.txt

# Install grpcio binary (for Triton client)
RUN pip install --no-cache-dir --only-binary :all: grpcio grpcio-tools

# Install Triton client for Python backend to call T3
RUN pip install --no-cache-dir tritonclient[grpc]

# Install dependencies for Chatterbox TTS
RUN pip install --no-cache-dir -c /tmp/constraints.txt \
        "transformers>=4.40.0" \
        tokenizers \
        safetensors \
        librosa soundfile \
        numpy scipy

# Install S3Gen/Chatterbox dependencies
RUN pip install --no-cache-dir -c /tmp/constraints.txt \
        omegaconf \
        einops \
        diffusers \
        conformer \
        s3tokenizer \
        huggingface_hub

# Install TensorRT Python bindings for TRTVocoder
# Check if tensorrt is already available, install if not
RUN python3 -c "import tensorrt" 2>/dev/null || \
    pip install --no-cache-dir tensorrt
