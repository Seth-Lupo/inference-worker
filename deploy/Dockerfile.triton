# Custom Triton image with TensorRT-LLM + Python dependencies
# Using Triton 25.12 with TRT-LLM 1.0.0+ (Qwen3 TensorRT engine support)
FROM nvcr.io/nvidia/tritonserver:25.12-trtllm-python-py3

# Install PyTorch with CUDA 13.0 support + torchaudio
RUN pip install --no-cache-dir \
        torch torchaudio --index-url https://download.pytorch.org/whl/cu130

# Install other Python packages for Parakeet ASR and CosyVoice TTS
RUN pip install --no-cache-dir \
        sherpa-onnx \
        onnxruntime-gpu \
        numpy \
        transformers \
        sentencepiece \
        hyperpyyaml \
        s3tokenizer \
        conformer \
        diffusers \
        onnx \
        openai-whisper \
        wget

# Install CosyVoice and Matcha-TTS from source
RUN git clone --recursive https://github.com/FunAudioLLM/CosyVoice.git /opt/CosyVoice && \
    cd /opt/CosyVoice && \
    pip install --no-cache-dir -e . && \
    cd third_party/Matcha-TTS && \
    pip install --no-cache-dir -e .

# Triton 25.12-trtllm-python-py3 includes:
# - TensorRT-LLM 1.0.0+ (Qwen3 TensorRT engine support)
# - Triton Inference Server with tensorrtllm + python backends
#
# IMPORTANT: Engines must be built with matching TRT-LLM version!
# Use build container: nvcr.io/nvidia/tritonserver:25.12-trtllm-python-py3

