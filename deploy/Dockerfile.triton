# =============================================================================
# Triton Inference Server - Pure Native Backends
# =============================================================================
#
# Models:
#   - qwen3: vLLM backend (LLM for chat/reasoning)
#   - t3: vLLM backend (TTS speech token generation)
#   - speech_encoder: ONNX backend (TTS reference audio → conditioning)
#   - conditional_decoder: ONNX backend (TTS speech tokens → audio)
#   - parakeet_encoder: ONNX backend (ASR encoder)
#   - parakeet_decoder: ONNX backend (ASR decoder)
#
# ONNX models from: ResembleAI/chatterbox-turbo-ONNX
# All orchestration done in Worker via gRPC.
#
# =============================================================================

FROM nvcr.io/nvidia/tritonserver:25.12-vllm-python-py3

# Add ONNX Runtime backend for TTS and ASR models
# Copy from the base py3 image which includes all backends
COPY --from=nvcr.io/nvidia/tritonserver:25.12-py3 \
    /opt/tritonserver/backends/onnxruntime \
    /opt/tritonserver/backends/onnxruntime

# Environment
ENV VLLM_USE_V1=0

# Minimal dependencies for vLLM custom models (T3)
RUN pip install --no-cache-dir --upgrade-strategy only-if-needed \
        "transformers>=4.45.0,<4.48.0" \
        "s3tokenizer>=0.3.0" && \
    rm -rf /root/.cache/pip

# Verify backends
RUN echo "Backends:" && ls -1 /opt/tritonserver/backends/
