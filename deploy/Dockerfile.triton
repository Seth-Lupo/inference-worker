# Triton Inference Server with vLLM + Python backends
# For serving:
#   - Qwen3 LLM (vLLM backend)
#   - T3 speech token generator (vLLM backend)
#   - Chatterbox TTS (Python backend with torch.compile)
#   - Parakeet ASR (Python backend with ONNX Runtime GPU)
#
# Base image already includes: torch, transformers, huggingface_hub, tokenizers,
# safetensors, numpy (via vLLM). We only add what's missing.
#
# Note: torchaudio removed - replaced with pure torch/librosa implementations
# to avoid ABI compatibility issues with base image torch.
#
# 25.05 requires Driver 575+ (compatible with Driver 580)
FROM nvcr.io/nvidia/tritonserver:25.05-vllm-python-py3

# Disable xformers to avoid triton kernel compatibility issues
ENV XFORMERS_DISABLE_MEMORY_EFFICIENT_ATTENTION=1
ENV DIFFUSERS_NO_XFORMERS=1

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
        espeak-ng \
        libsndfile1 \
        ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# Install Triton client for BLS (Python backend calling other Triton models)
RUN pip install --no-cache-dir tritonclient[grpc]

# CRITICAL: Freeze torch/torchvision from the base image to prevent ANY reinstallation.
# The NVIDIA container has a custom torch build - reinstalling from PyPI breaks vLLM ABI.
# We create a constraints file that pins the exact versions, preventing pip from touching them.
RUN pip freeze | grep -E "^(torch|torchvision|nvidia)" > /tmp/frozen-packages.txt && \
    echo "=== Frozen packages (will not be modified) ===" && \
    cat /tmp/frozen-packages.txt

# Install audio processing libraries
# --constraint ensures torch is never reinstalled
RUN pip install --no-cache-dir --constraint /tmp/frozen-packages.txt \
        librosa \
        soundfile \
        scipy

# Install S3Gen/Chatterbox dependencies (not in base image)
# diffusers 0.27+ avoids cached_download deprecation in new huggingface_hub
# Note: transformers, huggingface_hub, tokenizers, safetensors already in base via vLLM
RUN pip install --no-cache-dir --constraint /tmp/frozen-packages.txt \
        omegaconf \
        einops \
        "diffusers>=0.27.0,<0.30.0" \
        conformer \
        s3tokenizer

# Remove xformers to prevent triton kernel compatibility issues
# Env vars alone don't prevent module-level imports in diffusers
RUN pip uninstall -y xformers || true

# Install ONNX Runtime GPU for Parakeet ASR
# Uses CUDA EP for GPU acceleration of ONNX models
RUN pip install --no-cache-dir --constraint /tmp/frozen-packages.txt onnxruntime-gpu

# Verify torch wasn't modified
RUN echo "=== Final torch verification ===" && \
    python3 -c "import torch; print(f'torch: {torch.__version__}')" && \
    python3 -c "import vllm; print('vLLM import OK')"
