# Triton Inference Server with vLLM + Python backends
# For serving:
#   - Qwen3 LLM (vLLM backend)
#   - T3 speech token generator (vLLM backend)
#   - Chatterbox TTS (Python backend with torch.compile)
#   - Parakeet ASR (Python backend with ONNX Runtime GPU)
#
# Base image already includes: torch, transformers, huggingface_hub, tokenizers,
# safetensors, numpy (via vLLM). We only add what's missing.
#
# Note: torchaudio removed - replaced with pure torch/librosa implementations
# to avoid ABI compatibility issues with base image torch.
#
# Use base Triton Python container - install vLLM ourselves for ABI compatibility
# All packages from pip will have matching ABIs
FROM nvcr.io/nvidia/tritonserver:25.12-py3

# Disable xformers to avoid triton kernel compatibility issues
ENV XFORMERS_DISABLE_MEMORY_EFFICIENT_ATTENTION=1
ENV DIFFUSERS_NO_XFORMERS=1

# Force vLLM V0 engine - V1 cannot run in Triton's background threads
ENV VLLM_USE_V1=0

# Use Flash Attention backend (flashinfer not installed)
RUN pip install flashinfer \
    --extra-index-url https://flashinfer.ai/whl/cu130 \
    && pip install flashinfer-cubin \
    && pip install flashinfer-jit-cache \
    && flashinfer show-config

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
        espeak-ng \
        libsndfile1 \
        ffmpeg \
        git \
    && rm -rf /var/lib/apt/lists/*

# Fix corrupted numpy metadata in base container
RUN pip install --no-cache-dir --force-reinstall numpy

# Install Triton client for BLS (Python backend calling other Triton models)
RUN pip install --no-cache-dir tritonclient[grpc]

# Install vLLM 0.8.4 (matches Triton 25.05) + compressed-tensors for AWQ support
RUN pip install --no-cache-dir vllm==0.9.1 compressed-tensors

# Install vLLM backend for Triton (r25.05 matches vLLM 0.8.4)
RUN mkdir -p /opt/tritonserver/backends/vllm && \
    git clone --branch r25.05 --depth 1 https://github.com/triton-inference-server/vllm_backend.git /tmp/vllm_backend && \
    cp -r /tmp/vllm_backend/src/* /opt/tritonserver/backends/vllm && \
    rm -rf /tmp/vllm_backend

# Freeze torch and numpy to prevent later packages from changing them
RUN pip freeze | grep -E "^(torch|nvidia|numpy)" > /tmp/frozen-packages.txt && \
    echo "=== Frozen packages ===" && \
    cat /tmp/frozen-packages.txt

# Install audio processing libraries
# --constraint ensures torch is never reinstalled
RUN pip install --no-cache-dir --constraint /tmp/frozen-packages.txt \
        librosa \
        soundfile \
        scipy

# Pin transformers to avoid aimv2 conflict with s3tokenizer
# s3tokenizer registers model_type="aimv2", but transformers 4.48+ has built-in aimv2
RUN pip install --no-cache-dir --constraint /tmp/frozen-packages.txt \
        "transformers>=4.45.0,<4.48.0"

# Install S3Gen/Chatterbox dependencies (not in base image)
# diffusers 0.27+ avoids cached_download deprecation in new huggingface_hub
RUN pip install --no-cache-dir --constraint /tmp/frozen-packages.txt \
        omegaconf \
        einops \
        "diffusers>=0.27.0,<0.30.0" \
        conformer \
        "s3tokenizer>=0.3.0"

# Remove xformers if installed (causes triton kernel issues)
RUN pip uninstall -y xformers || true

# Install ONNX Runtime GPU for Parakeet ASR
# Uses CUDA EP for GPU acceleration of ONNX models
RUN pip install --no-cache-dir --constraint /tmp/frozen-packages.txt onnxruntime-gpu

# Verify installation
RUN echo "=== Installed versions ===" && \
    python3 -c "import numpy; print(f'numpy: {numpy.__version__}')" && \
    python3 -c "import torch; print(f'torch: {torch.__version__}')" && \
    python3 -c "import transformers; print(f'transformers: {transformers.__version__}')" && \
    python3 -c "import vllm; print(f'vLLM: {vllm.__version__}')" && \
    python3 -c "import compressed_tensors; print(f'compressed-tensors: {compressed_tensors.__version__}')" && \
    python3 -c "import s3tokenizer; print('s3tokenizer: OK')" && \
    ls -la /opt/tritonserver/backends/vllm/
