# =============================================================================
# Triton Inference Server - Pure Native Backends
# =============================================================================
#
# Models:
#   - qwen3: vLLM backend (LLM for chat/reasoning)
#   - t3: vLLM backend (TTS speech token generation)
#   - speech_encoder: ONNX backend (TTS reference audio → conditioning)
#   - conditional_decoder: ONNX backend (TTS speech tokens → audio)
#   - parakeet_encoder: ONNX backend (ASR encoder)
#   - parakeet_decoder: ONNX backend (ASR decoder)
#
# ONNX models from: ResembleAI/chatterbox-turbo-ONNX
# All orchestration done in Worker via gRPC.
#
# =============================================================================

FROM nvcr.io/nvidia/tritonserver:25.12-vllm-python-py3

# Environment
ENV VLLM_USE_V1=0

# Build ONNX Runtime backend from source
# Based on: https://github.com/triton-inference-server/onnxruntime_backend
ARG ONNXRUNTIME_VERSION=1.21.0
ARG TRITON_CONTAINER_VERSION=25.12

RUN apt-get update && apt-get install -y --no-install-recommends \
        cmake \
        rapidjson-dev \
    && rm -rf /var/lib/apt/lists/*

# Clone and build ONNX Runtime backend
RUN git clone --depth 1 --branch r${TRITON_CONTAINER_VERSION} \
        https://github.com/triton-inference-server/onnxruntime_backend.git \
        /tmp/onnxruntime_backend && \
    mkdir -p /tmp/onnxruntime_backend/build && \
    cd /tmp/onnxruntime_backend/build && \
    cmake -DCMAKE_INSTALL_PREFIX:PATH=/opt/tritonserver \
          -DTRITON_BUILD_ONNXRUNTIME_VERSION=${ONNXRUNTIME_VERSION} \
          -DTRITON_BUILD_CONTAINER_VERSION=${TRITON_CONTAINER_VERSION} \
          -DTRITON_ENABLE_GPU=ON \
          -DTRITON_ENABLE_ONNXRUNTIME_TENSORRT=OFF \
          .. && \
    make -j$(nproc) install && \
    rm -rf /tmp/onnxruntime_backend

# Minimal dependencies for vLLM custom models (T3)
RUN pip install --no-cache-dir --upgrade-strategy only-if-needed \
        "transformers>=4.45.0,<4.48.0" \
        "s3tokenizer>=0.3.0" && \
    rm -rf /root/.cache/pip

# Verify backends
RUN echo "Backends:" && ls -1 /opt/tritonserver/backends/
