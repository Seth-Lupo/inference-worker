# Custom Triton image with TensorRT-LLM + Python dependencies
# Using Triton 25.12 with TRT-LLM 1.0.0+ (Qwen3 TensorRT engine support)
FROM nvcr.io/nvidia/tritonserver:25.12-trtllm-python-py3

# Install system dependencies for audio processing
# espeak-ng: required by phonemizer for text-to-phoneme conversion
# libsndfile1: required by soundfile for audio I/O
RUN apt-get update && apt-get install -y --no-install-recommends \
        espeak-ng \
        libsndfile1 \
    && rm -rf /var/lib/apt/lists/*

# Install grpcio as pre-built binary (skip slow source compilation)
RUN pip install --no-cache-dir --only-binary :all: grpcio grpcio-tools

# Install PyTorch with CUDA 13.0 support + torchaudio
RUN pip install --no-cache-dir \
        torch torchaudio --index-url https://download.pytorch.org/whl/cu130

# Install Python packages for Parakeet ASR and CosyVoice TTS
# Based on CosyVoice requirements.txt + Matcha-TTS requirements.txt
RUN pip install --no-cache-dir \
        # ASR (Parakeet)
        sherpa-onnx \
        onnxruntime-gpu \
        # Core ML
        numpy \
        scipy \
        transformers \
        sentencepiece \
        einops \
        # CosyVoice/Matcha-TTS config
        hydra-core \
        omegaconf \
        hyperpyyaml \
        pydantic \
        # CosyVoice/Matcha-TTS audio processing
        librosa \
        soundfile \
        pyworld \
        phonemizer \
        inflect \
        Unidecode \
        # CosyVoice/Matcha-TTS models
        lightning \
        conformer \
        diffusers \
        x-transformers \
        # Utilities
        s3tokenizer \
        onnx \
        openai-whisper \
        wget \
        gdown

# Install CosyVoice and Matcha-TTS from source (not pip packages, just add to PYTHONPATH)
# Use --only-binary for grpcio to prevent slow source builds
RUN git clone --recursive https://github.com/FunAudioLLM/CosyVoice.git /opt/CosyVoice && \
    cd /opt/CosyVoice && \
    pip install --no-cache-dir --only-binary grpcio -r requirements.txt || true

# Add CosyVoice and Matcha-TTS to Python path
ENV PYTHONPATH="/opt/CosyVoice:/opt/CosyVoice/third_party/Matcha-TTS:${PYTHONPATH}"

# Fix LD_LIBRARY_PATH for onnxruntime-gpu to find CUDA libraries
ENV LD_LIBRARY_PATH="/usr/local/cuda/lib64:/usr/local/tensorrt/lib:/usr/lib/x86_64-linux-gnu:${LD_LIBRARY_PATH}"

# CUDA 13 compatibility: onnxruntime-gpu expects CUDA 12 libs, create symlinks
# This allows onnxruntime built for CUDA 12 to work with CUDA 13
RUN for lib in /usr/local/cuda/lib64/libcu*.so.13*; do \
        base=$(basename "$lib" | sed 's/\.13/.12/'); \
        if [ ! -e "/usr/local/cuda/lib64/$base" ]; then \
            ln -sf "$lib" "/usr/local/cuda/lib64/$base"; \
        fi; \
    done 2>/dev/null || true

# Triton 25.12-trtllm-python-py3 includes:
# - TensorRT-LLM 1.0.0+ (Qwen3 TensorRT engine support)
# - Triton Inference Server with tensorrtllm + python backends
#
# IMPORTANT: Engines must be built with matching TRT-LLM version!
# Use build container: nvcr.io/nvidia/tritonserver:25.12-trtllm-python-py3

